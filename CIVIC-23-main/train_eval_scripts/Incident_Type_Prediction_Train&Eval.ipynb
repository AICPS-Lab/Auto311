{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2Y4DKY5352sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TODO: Change this directory to point to CallDispatching-clean.xlsx. This file should satisfy the following requirements:\n",
        "# A \"descriptions\" column for data and a \"category\" column for its corresponding category.\n",
        "df = pd.read_excel(\"CallDispatching-clean.xlsx\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "lUr3jfrvwgLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the integers to be binary, and prints the categorization distribution result.\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(df[\"category\"].values), range(len(set(df[\"category\"].values))))\n",
        "}\n",
        "integer_encoding_map"
      ],
      "metadata": {
        "id": "BuEd6wO4xLAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_count = {}\n",
        "\n",
        "for key in integer_encoding_map.keys():\n",
        "    category_count[key] = 0\n",
        "\n",
        "for cate in df['category'].values:\n",
        "    category_count[cate] += 1\n",
        "\n",
        "print(category_count)\n",
        "\n",
        "temp_df = pd.DataFrame.from_dict(category_count, orient='index')\n",
        "temp_df.plot(kind='bar')"
      ],
      "metadata": {
        "id": "wYhu8VFXxTJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store the data into a .csv file based on its binary label.\n",
        "dataset = []\n",
        "\n",
        "for _, item in df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(12)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('call_dispatching.csv')"
      ],
      "metadata": {
        "id": "Pl5SbhVH2rTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qH798RAwY5q"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext==0.2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "RQyZLQS6AmWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section contains 6 models. We eventually adapted the Bert model as the most effective categorization model. See our paper for more details."
      ],
      "metadata": {
        "id": "3gs0mCld9wT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, in_channels, out_channels, kernel_heights, stride, padding, keep_probab, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(CNN, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of each batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\tin_channels : Number of input channels. Here it is 1 as the input data has dimension = (batch_size, num_seq, embedding_length)\n",
        "\t\tout_channels : Number of output channels after convolution operation performed on the input matrix\n",
        "\t\tkernel_heights : A list consisting of 3 different kernel_heights. Convolution will be performed 3 times and finally results from each kernel_height will be concatenated.\n",
        "\t\tkeep_probab : Probability of retaining an activation node during dropout operation\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embedding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
        "\t\t--------\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.in_channels = in_channels\n",
        "\t\tself.out_channels = out_channels\n",
        "\t\tself.kernel_heights = kernel_heights\n",
        "\t\tself.stride = stride\n",
        "\t\tself.padding = padding\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.conv1 = nn.Conv2d(in_channels, out_channels, (kernel_heights[0], embedding_length), stride, padding)\n",
        "\t\tself.conv2 = nn.Conv2d(in_channels, out_channels, (kernel_heights[1], embedding_length), stride, padding)\n",
        "\t\tself.conv3 = nn.Conv2d(in_channels, out_channels, (kernel_heights[2], embedding_length), stride, padding)\n",
        "\t\tself.dropout = nn.Dropout(keep_probab)\n",
        "\t\tself.label = nn.Linear(len(kernel_heights)*out_channels, output_size)\n",
        "\n",
        "\tdef conv_block(self, input, conv_layer):\n",
        "\t\tconv_out = conv_layer(input)# conv_out.size() = (batch_size, out_channels, dim, 1)\n",
        "\t\tactivation = F.relu(conv_out.squeeze(3))# activation.size() = (batch_size, out_channels, dim1)\n",
        "\t\tmax_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)# maxpool_out.size() = (batch_size, out_channels)\n",
        "\n",
        "\t\treturn max_out\n",
        "\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tThe idea of the Convolutional Neural Netwok for Text Classification is very simple. We perform convolution operation on the embedding matrix\n",
        "\t\twhose shape for each batch is (num_seq, embedding_length) with kernel of varying height but constant width which is same as the embedding_length.\n",
        "\t\tWe will be using ReLU activation after the convolution operation and then for each kernel height, we will use max_pool operation on each tensor\n",
        "\t\tand will filter all the maximum activation for every channel and then we will concatenate the resulting tensors. This output is then fully connected\n",
        "\t\tto the output layers consisting two units which basically gives us the logits for both positive and negative classes.\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentences: input_sentences of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class.\n",
        "\t\tlogits.size() = (batch_size, output_size)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\t# input.size() = (batch_size, num_seq, embedding_length)\n",
        "\t\tinput = input.unsqueeze(1)\n",
        "\t\t# input.size() = (batch_size, 1, num_seq, embedding_length)\n",
        "\t\tmax_out1 = self.conv_block(input, self.conv1)\n",
        "\t\tmax_out2 = self.conv_block(input, self.conv2)\n",
        "\t\tmax_out3 = self.conv_block(input, self.conv3)\n",
        "\n",
        "\t\tall_out = torch.cat((max_out1, max_out2, max_out3), 1)\n",
        "\t\t# all_out.size() = (batch_size, num_kernels*out_channels)\n",
        "\t\tfc_in = self.dropout(all_out)\n",
        "\t\t# fc_in.size()) = (batch_size, num_kernels*out_channels)\n",
        "\t\tlogits = self.label(fc_in)\n",
        "\n",
        "\t\treturn logits"
      ],
      "metadata": {
        "id": "Zh7hHhtyAoO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _*_ coding: utf-8 _*_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(LSTMClassifier, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\tdef forward(self, input_sentence, batch_size=None):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n",
        "\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
        "\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
        "\t\tfinal_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
        "\n",
        "\t\treturn final_output"
      ],
      "metadata": {
        "id": "gSh6WN1VAsmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _*_ coding: utf-8 _*_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "\n",
        "class AttentionModel(torch.nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(AttentionModel, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
        "\n",
        "\t\t--------\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\t\t#self.attn_fc_layer = nn.Linear()\n",
        "\n",
        "\tdef attention_net(self, lstm_output, final_state):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tNow we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\n",
        "\t\tbetween each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\n",
        "\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\n",
        "\t\tlstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
        "\t\tfinal_state : Final time-step hidden state (h_n) of the LSTM\n",
        "\n",
        "\t\t---------\n",
        "\n",
        "\t\tReturns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n",
        "\t\t\t\t  new hidden state.\n",
        "\n",
        "\t\tTensor Size :\n",
        "\t\t\t\t\thidden.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\tattn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tsoft_attn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tnew_hidden_state.size() = (batch_size, hidden_size)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\thidden = final_state.squeeze(0)\n",
        "\t\tattn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
        "\t\tsoft_attn_weights = F.softmax(attn_weights, 1)\n",
        "\t\tnew_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "\t\treturn new_hidden_state\n",
        "\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0)) # final_hidden_state.size() = (1, batch_size, hidden_size)\n",
        "\t\toutput = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n",
        "\n",
        "\t\tattn_output = self.attention_net(output, final_hidden_state)\n",
        "\t\tlogits = self.label(attn_output)\n",
        "\n",
        "\t\treturn logits"
      ],
      "metadata": {
        "id": "TOi9HVaXAvVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _*_ coding: utf-8 _*_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class RCNN(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(RCNN, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embedding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
        "\t\tself.dropout = 0.8\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n",
        "\t\tself.W2 = nn.Linear(2*hidden_size+embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\tdef forward(self, input_sentence, batch_size=None):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tThe idea of the paper \"Recurrent Convolutional Neural Networks for Text Classification\" is that we pass the embedding vector\n",
        "\t\tof the text sequences through a bidirectional LSTM and then for each sequence, our final embedding vector is the concatenation of\n",
        "\t\tits own GloVe embedding and the left and right contextual embedding which in bidirectional LSTM is same as the corresponding hidden\n",
        "\t\tstate. This final embedding is passed through a linear layer which maps this long concatenated encoding vector back to the hidden_size\n",
        "\t\tvector. After this step, we use a max pooling layer across all sequences of texts. This converts any varying length text into a fixed\n",
        "\t\tdimension tensor of size (batch_size, hidden_size) and finally we map this to the output layer.\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences, embedding_length)\n",
        "\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
        "\n",
        "\t\tfinal_encoding = torch.cat((output, input), 2).permute(1, 0, 2)\n",
        "\t\ty = self.W2(final_encoding) # y.size() = (batch_size, num_sequences, hidden_size)\n",
        "\t\ty = y.permute(0, 2, 1) # y.size() = (batch_size, hidden_size, num_sequences)\n",
        "\t\ty = F.max_pool1d(y, y.size()[2]) # y.size() = (batch_size, hidden_size, 1)\n",
        "\t\ty = y.squeeze(2)\n",
        "\t\tlogits = self.label(y)\n",
        "\n",
        "\t\treturn logits"
      ],
      "metadata": {
        "id": "3aARDvYkAzVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(RNN, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.rnn = nn.RNN(embedding_length, hidden_size, num_layers=2, bidirectional=True)\n",
        "\t\tself.label = nn.Linear(4*hidden_size, output_size)\n",
        "\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class which receives its input as the final_hidden_state of RNN.\n",
        "\t\tlogits.size() = (batch_size, output_size)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(4, self.batch_size, self.hidden_size).cuda()) # 4 = num_layers*num_directions\n",
        "\t\telse:\n",
        "\t\t\th_0 =  Variable(torch.zeros(4, batch_size, self.hidden_size).cuda())\n",
        "\t\toutput, h_n = self.rnn(input, h_0)\n",
        "\t\t# h_n.size() = (4, batch_size, hidden_size)\n",
        "\t\th_n = h_n.permute(1, 0, 2) # h_n.size() = (batch_size, 4, hidden_size)\n",
        "\t\th_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
        "\t\t# h_n.size() = (batch_size, 4*hidden_size)\n",
        "\t\tlogits = self.label(h_n) # logits.size() = (batch_size, output_size)\n",
        "\n",
        "\t\treturn logits"
      ],
      "metadata": {
        "id": "1zzdF0gJA05J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _*_ coding: utf-8 _*_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(SelfAttention, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
        "\n",
        "\t\t--------\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\tself.weights = weights\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.dropout = 0.8\n",
        "\t\tself.bilstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n",
        "\t\t# We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper\n",
        "\t\tself.W_s1 = nn.Linear(2*hidden_size, 350)\n",
        "\t\tself.W_s2 = nn.Linear(350, 30)\n",
        "\t\tself.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n",
        "\t\tself.label = nn.Linear(2000, output_size)\n",
        "\n",
        "\tdef attention_net(self, lstm_output):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tNow we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n",
        "\t\tencoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of\n",
        "\t\tthe input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully\n",
        "\t\tconnected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e.,\n",
        "\t\tpos & neg.\n",
        "\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\n",
        "\t\tlstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
        "\t\t---------\n",
        "\n",
        "\t\tReturns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
        "\t\t\t\t  attention to different parts of the input sentence.\n",
        "\n",
        "\t\tTensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t\t\t\t  attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tattn_weight_matrix = self.W_s2(F.tanh(self.W_s1(lstm_output)))\n",
        "\t\tattn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
        "\t\tattn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
        "\n",
        "\t\treturn attn_weight_matrix\n",
        "\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class.\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\n",
        "\t\toutput, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n",
        "\t\toutput = output.permute(1, 0, 2)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t# h_n.size() = (1, batch_size, hidden_size)\n",
        "\t\t# c_n.size() = (1, batch_size, hidden_size)\n",
        "\t\tattn_weight_matrix = self.attention_net(output)\n",
        "\t\t# attn_weight_matrix.size() = (batch_size, r, num_seq)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\thidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
        "\t\t# hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n",
        "\t\t# Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n",
        "\t\tfc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
        "\t\tlogits = self.label(fc_out)\n",
        "\t\t# logits.size() = (batch_size, output_size)\n",
        "\n",
        "\t\treturn logits"
      ],
      "metadata": {
        "id": "k3xNFgyyA3Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After initializing the models, we load the dataset and load the method for training the model."
      ],
      "metadata": {
        "id": "Hq2Bc5P6-ICW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# _*_ coding: utf-8 _*_\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "\n",
        "from torchtext.data import Field, Dataset, Example\n",
        "import pandas as pd\n",
        "\n",
        "class DataFrameDataset(Dataset):\n",
        "    def __init__(self, examples, fields, filter_pred=None):\n",
        "        self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
        "        if filter_pred is not None:\n",
        "            self.examples = filter(filter_pred, self.examples)\n",
        "        self.fields = dict(fields)\n",
        "\n",
        "        for n, f in list(self.fields.items()):\n",
        "            if isinstance(n, tuple):\n",
        "                self.fields.update(zip(n, f))\n",
        "                del self.fields[n]\n",
        "\n",
        "class SeriesExample(Example):\n",
        "    @classmethod\n",
        "    def fromSeries(cls, data, fields):\n",
        "        return cls.fromdict(data.to_dict(), fields)\n",
        "\n",
        "    @classmethod\n",
        "    def fromdict(cls, data, fields):\n",
        "        ex = cls()\n",
        "        for key, field in fields.items():\n",
        "            if key not in data:\n",
        "                raise ValueError(\"Specified key {} was not found in \"\n",
        "                     \"the input data\".format(key))\n",
        "            if field is not None:\n",
        "                setattr(ex, key, field.preprocess(data[key]))\n",
        "            else:\n",
        "                setattr(ex, key, data[key])\n",
        "        return ex\n",
        "\n",
        "\n",
        "def load_dataset(test_sen=None, path):\n",
        "\n",
        "    \"\"\"\n",
        "    tokenizer : Breaks sentences into a list of words. If sequential=False, no tokenization is applied\n",
        "    Field : A class that stores information about the way of preprocessing\n",
        "    fix_length : An important property of TorchText is that we can let the input to be variable length, and TorchText will\n",
        "                 dynamically pad each sequence to the longest sequence in that \"batch\". But here we are using fi_length which\n",
        "                 will pad each sequence to have a fix length of 200.\n",
        "\n",
        "    build_vocab : It will first make a vocabulary or dictionary mapping all the unique words present in the train_data to an\n",
        "                  idx and then after it will use GloVe word embedding to map the index to the corresponding word embedding.\n",
        "\n",
        "    vocab.vectors : This returns a torch tensor of shape (vocab_size x embedding_dim) containing the pre-trained word embeddings.\n",
        "    BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    tokenize = lambda x: x.split()\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\n",
        "    LABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
        "    datafields = { 'label' : LABEL, 'text' : TEXT }\n",
        "    torch_dataset = pd.read_csv(path)\n",
        "    # train_data, test_data = torch_dataset.split(split_ratio=[0.9, 0.1])\n",
        "\n",
        "    torch_dataset = DataFrameDataset(torch_dataset, datafields)\n",
        "\n",
        "    train_ds, test_ds = torch_dataset.split(split_ratio=[0.9, 0.1])\n",
        "\n",
        "    TEXT.build_vocab(train_ds, vectors=GloVe(name='6B', dim=300))\n",
        "    LABEL.build_vocab(test_ds)\n",
        "\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
        "    print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
        "    print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
        "\n",
        "    train_data, valid_data = train_ds.split() # Further splitting of training_data to create new training_data & validation_data\n",
        "    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_ds), batch_size=32, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
        "\n",
        "    '''Alternatively we can also use the default configurations'''\n",
        "    # train_iter, test_iter = datasets.IMDB.iters(batch_size=32)\n",
        "\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "\n",
        "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds"
      ],
      "metadata": {
        "id": "IihrzZzKBB4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad != None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "def train_model(model, train_iter, epoch):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.cuda()\n",
        "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    steps = 0\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        text = batch.text[0]\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        if torch.cuda.is_available():\n",
        "            text = text.cuda()\n",
        "            target = target.cuda()\n",
        "        if (text.size()[0] != 32):# One of the batch returned by BucketIterator has length different than 32.\n",
        "            continue\n",
        "        optim.zero_grad()\n",
        "        prediction = model(text)\n",
        "        loss = loss_fn(prediction, target)\n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(batch)\n",
        "        loss.backward()\n",
        "        clip_gradient(model, 1e-1)\n",
        "        optim.step()\n",
        "        steps += 1\n",
        "\n",
        "        if steps % 100 == 0:\n",
        "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
        "\n",
        "def eval_model(model, val_iter):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            text = batch.text[0]\n",
        "            if (text.size()[0] != 32):\n",
        "                continue\n",
        "            target = batch.label\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            # print(target)\n",
        "            if torch.cuda.is_available():\n",
        "                text = text.cuda()\n",
        "                target = target.cuda()\n",
        "            prediction = model(text)\n",
        "            # print(prediction)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter), predictions"
      ],
      "metadata": {
        "id": "bKztA09MCJKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the 12 categories. Each of them is used to binary classify whether the data belongs to this category. Since the code for each block is similar, I will include comments for only the first category, Minor Crash or Not."
      ],
      "metadata": {
        "id": "MynktK06-UTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minor Crash Or Not (Binary Classification)"
      ],
      "metadata": {
        "id": "Zj990kCt-_ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Redo the binary categorization\n",
        "crash_df = pd.read_excel(\"/content/drive/MyDrive/CIVIC-2023/CallDispatching/crash_or_not.xlsx\")\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(crash_df[\"category\"].values), range(len(set(crash_df[\"category\"].values))))\n",
        "}\n",
        "print(integer_encoding_map)"
      ],
      "metadata": {
        "id": "2b6Dvjp-_WD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to csv with the integer encoding\n",
        "dataset = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(2)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('crash_or_not.csv')"
      ],
      "metadata": {
        "id": "ymUH2Nix_uSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call to the pre-loaded function to load our dataset\n",
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds = load_dataset()"
      ],
      "metadata": {
        "id": "ew4s3BCXCB_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: adjust the parameter and select the appropriate model for training data\n",
        "learning_rate = 3e-4\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "\n",
        "# model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "model = CNN(batch_size=batch_size, output_size=output_size, in_channels=1, out_channels=1, kernel_heights=[3,6,9], stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, embedding_length=embedding_length, weights=word_embeddings)\n",
        "# model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "loss_fn = F.cross_entropy"
      ],
      "metadata": {
        "id": "FOksUHHnCFJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "# TODO: You can adjust the number of epochs here.\n",
        "\n",
        "for epoch in range(30):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc, _ = eval_model(model, valid_iter)\n",
        "    test_loss, test_acc, preds = eval_model(model, test_iter)\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "# test_loss, test_acc = eval_model(model, test_iter)\n",
        "# print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "XNAUoHGDCmXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crash_df.head()"
      ],
      "metadata": {
        "id": "Bqn76r1eFYAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "negative = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "    if item[\"category\"] == \"crash\":\n",
        "        positive.append({'label': 1, \"text\": item['descriptions']})\n",
        "    else:\n",
        "        negative.append({'label': 0, \"text\": item['descriptions']})"
      ],
      "metadata": {
        "id": "8XHTTNwjJGkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "PzxQLMzdI9G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allDataset = positive + negative\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))"
      ],
      "metadata": {
        "id": "JvPZVBGFJq3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse the training dataset\n",
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/crash_or_not/train_conf.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/crash_or_not/test_conf.csv\")"
      ],
      "metadata": {
        "id": "acnseW4wJ1o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "HHNZU5-uKCCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.dropout.p=0.5"
      ],
      "metadata": {
        "id": "8liop0BHKII9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "YUjLPc4EKNeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "BeiQkgIPKVTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Depending on the experiment purpose, add/ignore the dropout layer\n",
        "\n",
        "# model.dropout.train()\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=tokenized_datasets[\"train\"],\n",
        "#     eval_dataset=tokenized_datasets[\"test\"],\n",
        "#     compute_metrics=compute_metrics,\n",
        "# )\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "MTVko1QgYUZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "2L7e4ue4LF4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-crash-classification\")"
      ],
      "metadata": {
        "id": "6HfQ313eM1Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])"
      ],
      "metadata": {
        "id": "_AltZeyEM63-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates the confusion matrix\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "B0pKXqeEM8tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_frequent(nums):\n",
        "  counter = {}\n",
        "  for num in nums:\n",
        "    if num in counter:\n",
        "      counter[num] += 1\n",
        "    else:\n",
        "      counter[num] = 1\n",
        "\n",
        "  most_freq = max(counter, key=counter.get)\n",
        "  freq = counter[most_freq]\n",
        "\n",
        "  return most_freq, freq"
      ],
      "metadata": {
        "id": "kcv3UVNZlUCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "EZiCIc_0nD4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to('cuda')\n",
        "model.dropout.train()\n",
        "\n",
        "preds = []\n",
        "confs = []\n",
        "\n",
        "for i in tqdm(tokenized_datasets['test']):\n",
        "    text = i['text']\n",
        "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "    preds_list = []\n",
        "\n",
        "    for ct in range(100):\n",
        "        # print(ct)\n",
        "        prediction = torch.argmax(model(**encoded_input).logits)\n",
        "        preds_list.append((int(prediction.to(\"cpu\").tolist())))\n",
        "\n",
        "\n",
        "    finalized_pred, support = most_frequent(preds_list)\n",
        "    preds.append(finalized_pred)\n",
        "    confs.append(support/100)"
      ],
      "metadata": {
        "id": "E6GNIyzykxwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])"
      ],
      "metadata": {
        "id": "Ptb0a3MJnljd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confs[0])"
      ],
      "metadata": {
        "id": "PDWSTey4ntO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_preds = []\n",
        "new_golds = []\n",
        "\n",
        "for p, c, g in zip(preds, confs, golds):\n",
        "    if c >= 0.9:\n",
        "        new_preds.append(p)\n",
        "        new_golds.append(g)"
      ],
      "metadata": {
        "id": "igrwzHuas1gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(new_golds, new_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(new_golds, new_preds))"
      ],
      "metadata": {
        "id": "yGjEj4YjnYZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lost Or Not"
      ],
      "metadata": {
        "id": "v3NVkL1UOK0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crash_df = pd.read_excel(\"/content/drive/MyDrive/CIVIC-2023/CallDispatching/lost_or_not.xlsx\")\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(crash_df[\"category\"].values), range(len(set(crash_df[\"category\"].values))))\n",
        "}\n",
        "print(integer_encoding_map)"
      ],
      "metadata": {
        "id": "gwDqYSMAM_ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(2)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('lost_or_not.csv')"
      ],
      "metadata": {
        "id": "-J0-IbGFO8Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds = load_dataset()\n",
        "\n",
        "learning_rate = 3e-4\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "\n",
        "# model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "model = CNN(batch_size=batch_size, output_size=output_size, in_channels=1, out_channels=1, kernel_heights=[3,6,9], stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, embedding_length=embedding_length, weights=word_embeddings)\n",
        "# model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "loss_fn = F.cross_entropy"
      ],
      "metadata": {
        "id": "Gkbf3vbePAWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(30):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc, _ = eval_model(model, valid_iter)\n",
        "    test_loss, test_acc, preds = eval_model(model, test_iter)\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "# test_loss, test_acc = eval_model(model, test_iter)\n",
        "# print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "jwlpjQmePMqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.dropout.p=0.5"
      ],
      "metadata": {
        "id": "ETAE8jsWPXTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "negative = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "    if item[\"category\"] == \"lost\":\n",
        "        positive.append({'label': 1, \"text\": item['descriptions']})\n",
        "    else:\n",
        "        negative.append({'label': 0, \"text\": item['descriptions']})"
      ],
      "metadata": {
        "id": "crHVv9P5Pms0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "allDataset = positive + negative\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))"
      ],
      "metadata": {
        "id": "eD9hSRmzPu52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/lost_or_not/train_conf.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/lost_or_not/test_conf.csv\")"
      ],
      "metadata": {
        "id": "paHXELoUPzOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=3, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "VGdyTcCsQCRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "01-44sPrQHRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "Uri-5vJvQJy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-lost-classification-conf\")"
      ],
      "metadata": {
        "id": "5sF36Nw3Q4KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "2Qkq2tmlQ8hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to('cuda')\n",
        "model.dropout.train()\n",
        "\n",
        "preds = []\n",
        "confs = []\n",
        "\n",
        "for i in tqdm(tokenized_datasets['test']):\n",
        "    text = i['text']\n",
        "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "    preds_list = []\n",
        "\n",
        "    for ct in range(100):\n",
        "        # print(ct)\n",
        "        prediction = torch.argmax(model(**encoded_input).logits)\n",
        "        preds_list.append((int(prediction.to(\"cpu\").tolist())))\n",
        "\n",
        "\n",
        "    finalized_pred, support = most_frequent(preds_list)\n",
        "    preds.append(finalized_pred)\n",
        "    confs.append(support/100)"
      ],
      "metadata": {
        "id": "_VCqTxK9yGci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_preds = []\n",
        "new_golds = []\n",
        "\n",
        "for p, c, g in zip(preds, confs, golds):\n",
        "    if c >= 0.9:\n",
        "        new_preds.append(p)\n",
        "        new_golds.append(g)"
      ],
      "metadata": {
        "id": "t-qEkzphyIkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(new_golds, new_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(new_golds, new_preds))"
      ],
      "metadata": {
        "id": "AszRujZgyMwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aggressive Or Not"
      ],
      "metadata": {
        "id": "auLY3Bgqbc1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crash_df = pd.read_excel(\"/content/drive/MyDrive/CIVIC-2023/CallDispatching/aggressive_or_not.xlsx\")\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(crash_df[\"category\"].values), range(len(set(crash_df[\"category\"].values))))\n",
        "}\n",
        "print(integer_encoding_map)"
      ],
      "metadata": {
        "id": "OssgdCvtbh4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(2)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('aggressive_or_not.csv')"
      ],
      "metadata": {
        "id": "eieV2gaTcCYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds = load_dataset()\n",
        "\n",
        "learning_rate = 3e-4\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "\n",
        "# model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "model = CNN(batch_size=batch_size, output_size=output_size, in_channels=1, out_channels=1, kernel_heights=[3,6,9], stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, embedding_length=embedding_length, weights=word_embeddings)\n",
        "# model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "loss_fn = F.cross_entropy"
      ],
      "metadata": {
        "id": "f8Pu9XVycJKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(30):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc, _ = eval_model(model, valid_iter)\n",
        "    test_loss, test_acc, preds = eval_model(model, test_iter)\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "# test_loss, test_acc = eval_model(model, test_iter)\n",
        "# print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "wme8BijwcOeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "negative = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "    if item[\"category\"] == \"aggressive driver\":\n",
        "        positive.append({'label': 1, \"text\": item['descriptions']})\n",
        "    else:\n",
        "        negative.append({'label': 0, \"text\": item['descriptions']})"
      ],
      "metadata": {
        "id": "0ZK5sXl0cXsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "allDataset = positive + negative\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))"
      ],
      "metadata": {
        "id": "5W7nSMq4csyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/aggressive_or_not/train.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/aggressive_or_not/test.csv\")"
      ],
      "metadata": {
        "id": "uxgtQKYNctcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "id": "Q1OGFWb7cUCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "tahX16ZCc1M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "IKOsaZCIc5LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "AoZ7nk7Xc6o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-aggressive-classification\")"
      ],
      "metadata": {
        "id": "HUjvJB46eEQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "pI0AU4NoeF9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check Welfare or not"
      ],
      "metadata": {
        "id": "IBplXU5peUvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crash_df = pd.read_excel(\"/content/drive/MyDrive/CIVIC-2023/CallDispatching/welfare_or_not.xlsx\")\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(crash_df[\"category\"].values), range(len(set(crash_df[\"category\"].values))))\n",
        "}\n",
        "print(integer_encoding_map)\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(2)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('welfare_or_not.csv')\n",
        "\n",
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds = load_dataset()\n",
        "\n",
        "learning_rate = 3e-4\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "\n",
        "# model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "model = CNN(batch_size=batch_size, output_size=output_size, in_channels=1, out_channels=1, kernel_heights=[3,6,9], stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, embedding_length=embedding_length, weights=word_embeddings)\n",
        "# model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "loss_fn = F.cross_entropy\n",
        "\n",
        "for epoch in range(30):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc, _ = eval_model(model, valid_iter)\n",
        "    test_loss, test_acc, preds = eval_model(model, test_iter)\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "# test_loss, test_acc = eval_model(model, test_iter)\n",
        "# print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "YaMaxQVeeZE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "negative = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "    if item[\"category\"] == \"check welfare\":\n",
        "        positive.append({'label': 1, \"text\": item['descriptions']})\n",
        "    else:\n",
        "        negative.append({'label': 0, \"text\": item['descriptions']})\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "allDataset = positive + negative\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))\n",
        "\n",
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/welfare_or_not/train.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/welfare_or_not/test.csv\")"
      ],
      "metadata": {
        "id": "8wrh1fafh3Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "vFMiaplJiBR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n",
        "\n",
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "BOe_NrjRiQ9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-welfare-classification\")"
      ],
      "metadata": {
        "id": "LtA65LlaiSu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Damaged Property or Not"
      ],
      "metadata": {
        "id": "Zd-ZUUyYi5RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crash_df = pd.read_excel(\"/content/drive/MyDrive/CIVIC-2023/CallDispatching/damaged_or_not.xlsx\")\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(crash_df[\"category\"].values), range(len(set(crash_df[\"category\"].values))))\n",
        "}\n",
        "print(integer_encoding_map)\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(2)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('damaged_or_not.csv')\n",
        "\n",
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds = load_dataset()\n",
        "\n",
        "learning_rate = 3e-4\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "\n",
        "# model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "model = CNN(batch_size=batch_size, output_size=output_size, in_channels=1, out_channels=1, kernel_heights=[3,6,9], stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, embedding_length=embedding_length, weights=word_embeddings)\n",
        "# model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "loss_fn = F.cross_entropy\n",
        "\n",
        "for epoch in range(30):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc, _ = eval_model(model, valid_iter)\n",
        "    test_loss, test_acc, preds = eval_model(model, test_iter)\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "# test_loss, test_acc = eval_model(model, test_iter)\n",
        "# print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "aZj9xOPFi8QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "negative = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "    if item[\"category\"] == \"damaged property\":\n",
        "        positive.append({'label': 1, \"text\": item['descriptions']})\n",
        "    else:\n",
        "        negative.append({'label': 0, \"text\": item['descriptions']})\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "allDataset = positive + negative\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))\n",
        "\n",
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/damaged_or_not/train.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/damaged_or_not/test.csv\")"
      ],
      "metadata": {
        "id": "LEiydR-tjGnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "3d6VMid2jPYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n",
        "\n",
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "EkPmo4aDjUUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-damaged-classification\")"
      ],
      "metadata": {
        "id": "Bupu8RBjkbDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noise or Not"
      ],
      "metadata": {
        "id": "cxiBOyJIkka3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crash_df = pd.read_excel(\"/content/drive/MyDrive/CIVIC-2023/CallDispatching/noise_or_not.xlsx\")\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(crash_df[\"category\"].values), range(len(set(crash_df[\"category\"].values))))\n",
        "}\n",
        "print(integer_encoding_map)\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(2)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('noise_or_not.csv')\n",
        "\n",
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds = load_dataset()\n",
        "\n",
        "learning_rate = 3e-4\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "\n",
        "# model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "model = CNN(batch_size=batch_size, output_size=output_size, in_channels=1, out_channels=1, kernel_heights=[3,6,9], stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, embedding_length=embedding_length, weights=word_embeddings)\n",
        "# model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "loss_fn = F.cross_entropy\n",
        "\n",
        "for epoch in range(30):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc, _ = eval_model(model, valid_iter)\n",
        "    test_loss, test_acc, preds = eval_model(model, test_iter)\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "# test_loss, test_acc = eval_model(model, test_iter)\n",
        "# print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "tD2VKRt5kmNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "negative = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "    if item[\"category\"] == \"noise violation\":\n",
        "        positive.append({'label': 1, \"text\": item['descriptions']})\n",
        "    else:\n",
        "        negative.append({'label': 0, \"text\": item['descriptions']})\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "allDataset = positive + negative\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))\n",
        "\n",
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/damaged_or_not/train.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/damaged_or_not/test.csv\")"
      ],
      "metadata": {
        "id": "dQXtcy_Dk20E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "rHVGbDQJk72f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n",
        "\n",
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "mT20UIctldL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-noise-classification\")"
      ],
      "metadata": {
        "id": "znJmF49ElefP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Roadway or Not"
      ],
      "metadata": {
        "id": "at8HKguhls22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crash_df = pd.read_excel(\"/content/roadway_or_not.xlsx\")\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(crash_df[\"category\"].values), range(len(set(crash_df[\"category\"].values))))\n",
        "}\n",
        "print(integer_encoding_map)\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(2)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('roadway_or_not.csv')\n",
        "\n",
        "# TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds = load_dataset()\n",
        "\n",
        "# learning_rate = 3e-4\n",
        "# batch_size = 32\n",
        "# output_size = 2\n",
        "# hidden_size = 256\n",
        "# embedding_length = 300\n",
        "\n",
        "# # model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = CNN(batch_size=batch_size, output_size=output_size, in_channels=1, out_channels=1, kernel_heights=[3,6,9], stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, embedding_length=embedding_length, weights=word_embeddings)\n",
        "# # model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# loss_fn = F.cross_entropy\n",
        "\n",
        "# for epoch in range(30):\n",
        "#     train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "#     val_loss, val_acc, _ = eval_model(model, valid_iter)\n",
        "#     test_loss, test_acc, preds = eval_model(model, test_iter)\n",
        "#     print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "# test_loss, test_acc = eval_model(model, test_iter)\n",
        "# print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "oVW6LcoAlusR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "_MjvRgeXD-5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "negative = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "    if item[\"category\"] == \"roadway hazard\":\n",
        "        positive.append({'label': 1, \"text\": item['descriptions']})\n",
        "    else:\n",
        "        negative.append({'label': 0, \"text\": item['descriptions']})\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "allDataset = positive + negative\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))\n",
        "\n",
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/roadway_or_not/train.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/roadway_or_not/test.csv\")"
      ],
      "metadata": {
        "id": "eZ1--Kp2l6Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "lnw79kjPEHLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "E6EIOhhPmE19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainer.evaluate())\n",
        "\n",
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "h7hPboU8mIE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-roadway-classification\")"
      ],
      "metadata": {
        "id": "SuaYYz7LmUve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abandoned or Not"
      ],
      "metadata": {
        "id": "34ayMzhdmZu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crash_df = pd.read_excel(\"/content/abandoned_or_not.xlsx\")\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(crash_df[\"category\"].values), range(len(set(crash_df[\"category\"].values))))\n",
        "}\n",
        "print(integer_encoding_map)\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(2)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('abandoned_or_not.csv')\n",
        "\n",
        "# TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds = load_dataset()\n",
        "\n",
        "# learning_rate = 3e-4\n",
        "# batch_size = 32\n",
        "# output_size = 2\n",
        "# hidden_size = 256\n",
        "# embedding_length = 300\n",
        "\n",
        "# # model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = CNN(batch_size=batch_size, output_size=output_size, in_channels=1, out_channels=1, kernel_heights=[3,6,9], stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, embedding_length=embedding_length, weights=word_embeddings)\n",
        "# # model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# loss_fn = F.cross_entropy\n",
        "\n",
        "# for epoch in range(30):\n",
        "#     train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "#     val_loss, val_acc, _ = eval_model(model, valid_iter)\n",
        "#     test_loss, test_acc, preds = eval_model(model, test_iter)\n",
        "#     print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "# test_loss, test_acc = eval_model(model, test_iter)\n",
        "# print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "rbtPuEltmebL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "negative = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "    if item[\"category\"] == \"abandoned vehicles\":\n",
        "        positive.append({'label': 1, \"text\": item['descriptions']})\n",
        "    else:\n",
        "        negative.append({'label': 0, \"text\": item['descriptions']})\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "allDataset = positive + negative\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))\n",
        "\n",
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/abandoned_or_not/train.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/abandoned_or_not/test.csv\")"
      ],
      "metadata": {
        "id": "XewHx4z8moNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "CTzBoYGYm8FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainer.evaluate())\n",
        "\n",
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "PEZG_bs7nAGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-abandoned-classification\")"
      ],
      "metadata": {
        "id": "_WG3IpGRnCRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drug or Not"
      ],
      "metadata": {
        "id": "WyGVwICNoIC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crash_df = pd.read_excel(\"/content/drug_or_not.xlsx\")\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(crash_df[\"category\"].values), range(len(set(crash_df[\"category\"].values))))\n",
        "}\n",
        "print(integer_encoding_map)\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(2)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('drug_or_not.csv')\n",
        "\n",
        "# TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds = load_dataset()\n",
        "\n",
        "# learning_rate = 3e-4\n",
        "# batch_size = 32\n",
        "# output_size = 2\n",
        "# hidden_size = 256\n",
        "# embedding_length = 300\n",
        "\n",
        "# # model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = CNN(batch_size=batch_size, output_size=output_size, in_channels=1, out_channels=1, kernel_heights=[3,6,9], stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, embedding_length=embedding_length, weights=word_embeddings)\n",
        "# # model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# loss_fn = F.cross_entropy\n",
        "\n",
        "# for epoch in range(30):\n",
        "#     train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "#     val_loss, val_acc, _ = eval_model(model, valid_iter)\n",
        "#     test_loss, test_acc, preds = eval_model(model, test_iter)\n",
        "#     print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "# test_loss, test_acc = eval_model(model, test_iter)\n",
        "# print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "MzXd4IAvoWjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "negative = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "    if item[\"category\"] == \"drug or prostitution activity\":\n",
        "        positive.append({'label': 1, \"text\": item['descriptions']})\n",
        "    else:\n",
        "        negative.append({'label': 0, \"text\": item['descriptions']})\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "allDataset = positive + negative\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))\n",
        "\n",
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/drug_or_not/train.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/drug_or_not/test.csv\")"
      ],
      "metadata": {
        "id": "yw2DsyVCokDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Dz62Knkhot1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n",
        "\n",
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "RnEWPpNBou7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-drug-classification\")"
      ],
      "metadata": {
        "id": "IDydMf_opX1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Animal or Not"
      ],
      "metadata": {
        "id": "QLxCCKdnoW3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crash_df = pd.read_excel(\"/content/animal_or_not.xlsx\")\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(crash_df[\"category\"].values), range(len(set(crash_df[\"category\"].values))))\n",
        "}\n",
        "print(integer_encoding_map)\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(2)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('animal_or_not.csv')\n",
        "\n",
        "# TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds = load_dataset()\n",
        "\n",
        "# learning_rate = 3e-4\n",
        "# batch_size = 32\n",
        "# output_size = 2\n",
        "# hidden_size = 256\n",
        "# embedding_length = 300\n",
        "\n",
        "# # model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = CNN(batch_size=batch_size, output_size=output_size, in_channels=1, out_channels=1, kernel_heights=[3,6,9], stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, embedding_length=embedding_length, weights=word_embeddings)\n",
        "# # model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# loss_fn = F.cross_entropy\n",
        "\n",
        "# for epoch in range(30):\n",
        "#     train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "#     val_loss, val_acc, _ = eval_model(model, valid_iter)\n",
        "#     test_loss, test_acc, preds = eval_model(model, test_iter)\n",
        "#     print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "# test_loss, test_acc = eval_model(model, test_iter)\n",
        "# print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "t_qGWrx6oZJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "negative = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "    if item[\"category\"] == \"animal\":\n",
        "        positive.append({'label': 1, \"text\": item['descriptions']})\n",
        "    else:\n",
        "        negative.append({'label': 0, \"text\": item['descriptions']})\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "allDataset = positive + negative\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))\n",
        "\n",
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/animal_or_not/train.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/animal_or_not/test.csv\")"
      ],
      "metadata": {
        "id": "nHxFgsIBphFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Tiz19uMSpoDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n",
        "\n",
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "MK8KVzBPpsnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-animal-classification\")"
      ],
      "metadata": {
        "id": "8Yaq-vlmptNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Illegal Parking or Not"
      ],
      "metadata": {
        "id": "J-0Hx0y5oZgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crash_df = pd.read_excel(\"/content/illegalparking_or_not.xlsx\")\n",
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(crash_df[\"category\"].values), range(len(set(crash_df[\"category\"].values))))\n",
        "}\n",
        "print(integer_encoding_map)\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "\n",
        "    blank = [0 for _ in range(2)]\n",
        "    idx = integer_encoding_map[item['category']]\n",
        "    blank[idx] = 1\n",
        "    dataset.append({'text': item['descriptions'], 'label': blank})\n",
        "\n",
        "\n",
        "temp_df = pd.DataFrame(dataset)\n",
        "temp_df.head()\n",
        "\n",
        "temp_df.to_csv('illegalparking_or_not.csv')\n",
        "\n",
        "# TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter, test_ds = load_dataset()\n",
        "\n",
        "# learning_rate = 3e-4\n",
        "# batch_size = 32\n",
        "# output_size = 2\n",
        "# hidden_size = 256\n",
        "# embedding_length = 300\n",
        "\n",
        "# # model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# model = CNN(batch_size=batch_size, output_size=output_size, in_channels=1, out_channels=1, kernel_heights=[3,6,9], stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, embedding_length=embedding_length, weights=word_embeddings)\n",
        "# # model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# # model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "# loss_fn = F.cross_entropy\n",
        "\n",
        "# for epoch in range(30):\n",
        "#     train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "#     val_loss, val_acc, _ = eval_model(model, valid_iter)\n",
        "#     test_loss, test_acc, preds = eval_model(model, test_iter)\n",
        "#     print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "# test_loss, test_acc = eval_model(model, test_iter)\n",
        "# print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "sfMZ57_eqWtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = []\n",
        "negative = []\n",
        "\n",
        "for _, item in crash_df.iterrows():\n",
        "    if item[\"category\"] == \"illegal parking\":\n",
        "        positive.append({'label': 1, \"text\": item['descriptions']})\n",
        "    else:\n",
        "        negative.append({'label': 0, \"text\": item['descriptions']})\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "allDataset = positive + negative\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))\n",
        "\n",
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/illegalparking_or_not/train.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/illegalparking_or_not/test.csv\")"
      ],
      "metadata": {
        "id": "6ybJ9QO9qgec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "eefRmi01qdbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n",
        "\n",
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "ZlP1eLS2qoJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-illegalparking-classification\")"
      ],
      "metadata": {
        "id": "ZgbE-6JxqsPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Others-Major"
      ],
      "metadata": {
        "id": "6xM5mPuxRR1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "major_df = pd.read_excel(\"/content/drive/MyDrive/CIVIC-2023/CallDispatching/others-major.xlsx\")\n",
        "major_df.head()"
      ],
      "metadata": {
        "id": "I2lZYbHOWCwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(major_df[\"category\"].values), range(len(set(major_df[\"category\"].values))))\n",
        "}\n",
        "integer_encoding_map"
      ],
      "metadata": {
        "id": "uKVHfF6-WLrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggressive_driver = []\n",
        "check_welfare = []\n",
        "damaged_property = []"
      ],
      "metadata": {
        "id": "Rss1Td6xWM-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _, item in major_df.iterrows():\n",
        "    if item['category'] == \"aggressive driver\":\n",
        "        aggressive_driver.append({\"label\": integer_encoding_map[item[\"category\"]], \"text\": item[\"descriptions\"]})\n",
        "\n",
        "    if item['category'] == \"check welfare\":\n",
        "        check_welfare.append({\"label\": integer_encoding_map[item[\"category\"]], \"text\": item[\"descriptions\"]})\n",
        "\n",
        "    if item['category'] == \"damaged property\":\n",
        "        damaged_property.append({\"label\": integer_encoding_map[item[\"category\"]], \"text\": item[\"descriptions\"]})"
      ],
      "metadata": {
        "id": "7PvfdKOOXKaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allDataset = aggressive_driver + check_welfare + damaged_property\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))"
      ],
      "metadata": {
        "id": "6DVQmDAiXtYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/others-major/train.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/others-major/test.csv\")"
      ],
      "metadata": {
        "id": "H0A6a1IAXzo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)"
      ],
      "metadata": {
        "id": "y_WgQV6WX3wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=3, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "TxccJq0vX9Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "_3uN9246YDGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "dXVvLlccYELI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/bert-others-major-classification\")"
      ],
      "metadata": {
        "id": "V1Y4zHrVYThE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_report = trainer.predict(tokenized_datasets['test'])\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in preds_report[0]:\n",
        "  preds.append(np.argmax(i))\n",
        "\n",
        "golds = []\n",
        "\n",
        "for i in tokenized_datasets[\"test\"]:\n",
        "  golds.append(i['label'])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(golds, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "print(classification_report(golds, preds))"
      ],
      "metadata": {
        "id": "5ZB_mPqwYZcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Others-Minor"
      ],
      "metadata": {
        "id": "nsoBS2ODYnLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "minor_df = pd.read_excel(\"/content/drive/MyDrive/CIVIC-2023/CallDispatching/others-minor.xlsx\")\n",
        "minor_df.head()"
      ],
      "metadata": {
        "id": "on0ciwyOYfAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "integer_encoding_map = {\n",
        "    i:j for i, j in zip(set(minor_df[\"category\"].values), range(len(set(minor_df[\"category\"].values))))\n",
        "}\n",
        "integer_encoding_map"
      ],
      "metadata": {
        "id": "2bvNh0E1YwNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_pros = []\n",
        "animal = []\n",
        "illegal_parking = []\n",
        "abandoned_vehicle = []\n",
        "found_property = []\n",
        "roadway_harzard = []\n",
        "noise_violation = []"
      ],
      "metadata": {
        "id": "S7XdoLFdY8fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _, item in minor_df.iterrows():\n",
        "    if item['category'] == \"drug or prostitution activity\":\n",
        "        drug_pros.append({\"label\": integer_encoding_map[item[\"category\"]], \"text\": item[\"descriptions\"]})\n",
        "\n",
        "    if item['category'] == \"animal\":\n",
        "        animal.append({\"label\": integer_encoding_map[item[\"category\"]], \"text\": item[\"descriptions\"]})\n",
        "\n",
        "    if item['category'] == \"illegal parking\":\n",
        "        illegal_parking.append({\"label\": integer_encoding_map[item[\"category\"]], \"text\": item[\"descriptions\"]})\n",
        "\n",
        "    if item['category'] == \"abandoned vehicles\":\n",
        "        abandoned_vehicle.append({\"label\": integer_encoding_map[item[\"category\"]], \"text\": item[\"descriptions\"]})\n",
        "\n",
        "    if item['category'] == \"found property\":\n",
        "        found_property.append({\"label\": integer_encoding_map[item[\"category\"]], \"text\": item[\"descriptions\"]})\n",
        "\n",
        "    if item['category'] == \"roadway hazard\":\n",
        "        roadway_harzard.append({\"label\": integer_encoding_map[item[\"category\"]], \"text\": item[\"descriptions\"]})\n",
        "\n",
        "    if item['category'] == \"noise violation\":\n",
        "        noise_violation.append({\"label\": integer_encoding_map[item[\"category\"]], \"text\": item[\"descriptions\"]})"
      ],
      "metadata": {
        "id": "rgxolxe-Y0Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allDataset = drug_pros + animal + illegal_parking + abandoned_vehicle + found_property + roadway_harzard + noise_violation\n",
        "allDataset = pd.DataFrame(allDataset)\n",
        "\n",
        "allDataset = Dataset.from_pandas(allDataset)\n",
        "allDataset = allDataset.train_test_split(test_size=0.2)\n",
        "allDataset['train']\n",
        "\n",
        "print(set(allDataset['train']['label']))\n",
        "print(set(allDataset['test']['label']))"
      ],
      "metadata": {
        "id": "p4d51wNKZd-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allDataset['train'].to_csv(\"/content/drive/MyDrive/311_datasets/others-minor/train.csv\")\n",
        "allDataset['test'].to_csv(\"/content/drive/MyDrive/311_datasets/others-minor/test.csv\")"
      ],
      "metadata": {
        "id": "8bHJxYeyZp5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = allDataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "70v1IobUZ8dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=7)"
      ],
      "metadata": {
        "id": "MrvAB3XAaCey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.dropout"
      ],
      "metadata": {
        "id": "PN9RGnpIaEzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"test_trainer\", logging_steps=1)\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=10, logging_steps=1)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"train\"])))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=2023).select(range(len(tokenized_datasets[\"test\"])))\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "bHGgzHfZaIIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "POGv8RryaM9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDvtrXGPaOx_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}